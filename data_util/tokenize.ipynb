{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b76d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from ckipnlp.driver.ss import CkipSentenceSegmenter as SentSeg\n",
    "# Make sentence segmentation obj (data structure that ckip tagger use)\n",
    "from ckiptagger import construct_dictionary, WS, POS, NER\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1371c973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3dabff5350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3dabff5350>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f41601a1350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f41601a1350>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function POS.__del__ at 0x7f417f258cb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/guest/r08944052/anaconda3/envs/pg3/lib/python3.7/site-packages/ckiptagger/api.py\", line 185, in __del__\n",
      "    self.model.sess.close()\n",
      "AttributeError: 'POS' object has no attribute 'model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f41601a1fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f41601a1fd0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f4160118190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f4160118190>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3c5e4b2f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3c5e4b2f50>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3d643c7050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3d643c7050>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3d6436e3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3d6436e3d0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3d64380cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3d64380cd0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f417d4ea950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f417d4ea950>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3daaa1dd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3daaa1dd90>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3d51958f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3d51958f50>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3d5191c1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3d5191c1d0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3d518c7f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3d518c7f50>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3d518b5b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f3d518b5b10>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3d63712b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3d63712b90>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    }
   ],
   "source": [
    "# init pipeline\n",
    "ss = SentSeg(delims = {\",\", \"。\", \":\", \"?\", \"!\", \";\"})\n",
    "ws = WS(\"../ckip\", disable_cuda=False)\n",
    "pos = POS(\"../ckip\", disable_cuda=False)\n",
    "ner = NER(\"../ckip\", disable_cuda=False)\n",
    "\n",
    "NER_LABLE = ['GPE', 'PERSON', 'DATE', 'EVENT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9dbb50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pth = '../data'\n",
    "train = np.load(os.path.join(data_pth, 'train_ner.npy'), allow_pickle=True)\n",
    "test = np.load(os.path.join(data_pth, 'test_ner.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b746cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_words(ws_lst):\n",
    "    all_token = [' '.join(ws_sent) for ws_sent in ws_lst]\n",
    "    all_sent = ' '.join(all_token).strip()\n",
    "    return all_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94641922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(dataset):\n",
    "    for i, data in tqdm(enumerate(dataset)):\n",
    "        art_seg = ss(raw = data['content'])\n",
    "        art_ws_lst = ws(art_seg)\n",
    "        \n",
    "        title_seg = ss(raw = data['title'])\n",
    "        title_ws_lst = ws(title_seg)\n",
    "        \n",
    "        art_pos_lst = pos(art_ws_lst)\n",
    "        ner_lst = ner(art_ws_lst, art_pos_lst)\n",
    "        ner_set = []\n",
    "        \n",
    "        dataset[i]['token_content'] = get_token_words(art_ws_lst)\n",
    "        dataset[i]['token_title'] = get_token_words(title_ws_lst)\n",
    "        \n",
    "        for j, sentence in enumerate(art_seg):\n",
    "            for entity in sorted(ner_lst[j]):\n",
    "                if entity[2] in NER_LABLE:\n",
    "                    ner_w = entity[-2:]\n",
    "                    if ner_w not in ner_set:\n",
    "                        ner_set.append(entity[-2:])\n",
    "                        \n",
    "        ner_tgt = ' '.join([x[1] for x in ner_set])\n",
    "    \n",
    "        dataset[i]['ner'] = ner_set\n",
    "        dataset[i]['ner_tgt'] = ner_tgt\n",
    "        \n",
    "        del art_seg, art_ws_lst, title_seg, title_ws_lst, art_pos_lst, ner_lst, ner_set\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92ec10d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4581it [1:04:10,  1.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-7c84389fdba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-4be15ab60aef>\u001b[0m in \u001b[0;36mtokenizer\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mart_pos_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart_ws_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mner_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart_ws_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mart_pos_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mner_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pg3/lib/python3.7/site-packages/ckiptagger/api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, word_sentence_list, pos_sentence_list, character_normalization, batch_sentences, batch_characters)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0mparital_label_sentence_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mparital_label_sentence_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_label_for_a_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpartial_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mlabel_sentence_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparital_label_sentence_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpartial_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pg3/lib/python3.7/site-packages/ckiptagger/model_ner.py\u001b[0m in \u001b[0;36mpredict_label_for_a_batch\u001b[0;34m(self, sample_list)\u001b[0m\n\u001b[1;32m    424\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_v\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc_v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_v\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw_v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m             }\n\u001b[1;32m    428\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/pg3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pg3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pg3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pg3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pg3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pg3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_token = tokenizer(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528193c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/train_token.npy', train_token)\n",
    "np.save('../data/test_token.npy', train_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98291630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import collections\n",
    "from tensorflow.core.example import example_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f86e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50_000  # 词汇表大小\n",
    "CHUNK_SIZE = 1000    # 每个分块example的数量，用于分块的数据\n",
    "\n",
    "# tf模型数据文件存放目录\n",
    "FINISHED_FILE_DIR = '../data/finished_files'\n",
    "CHUNKS_DIR = os.path.join(FINISHED_FILE_DIR, 'chunked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa9bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_file(finished_files_dir, chunks_dir, name, chunk_size):\n",
    "    \"\"\"构建二进制文件\"\"\"\n",
    "    in_file = os.path.join(finished_files_dir, '%s.bin' % name)\n",
    "    print(in_file)\n",
    "    reader = open(in_file, \"rb\")\n",
    "    chunk = 0\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        chunk_fname = os.path.join(chunks_dir, '%s_%03d.bin' % (name, chunk))  # 新的分块\n",
    "        with open(chunk_fname, 'wb') as writer:\n",
    "            for _ in range(chunk_size):\n",
    "                len_bytes = reader.read(8)\n",
    "                if not len_bytes:\n",
    "                    finished = True\n",
    "                    break\n",
    "                str_len = struct.unpack('q', len_bytes)[0]\n",
    "                example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "                writer.write(struct.pack('q', str_len))\n",
    "                writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "            chunk += 1\n",
    "\n",
    "\n",
    "def chunk_all():\n",
    "    # 创建一个文件夹来保存分块\n",
    "    if not os.path.isdir(CHUNKS_DIR):\n",
    "        os.mkdir(CHUNKS_DIR)\n",
    "    # 将数据分块\n",
    "    for name in ['train', 'val']:\n",
    "        print(\"Splitting %s data into chunks...\" % name)\n",
    "        chunk_file(FINISHED_FILE_DIR, CHUNKS_DIR, name, CHUNK_SIZE)\n",
    "    print(\"Saved chunked data in %s\" % CHUNKS_DIR)\n",
    "\n",
    "\n",
    "def write_to_bin(input_file, out_file, makevocab=False):\n",
    "    \"\"\"生成模型需要的文件\"\"\"\n",
    "    if makevocab:\n",
    "        vocab_counter = collections.Counter()\n",
    "\n",
    "    with open(out_file, 'wb') as writer:\n",
    "        \n",
    "        for data in input_file:\n",
    "                article = data['token_content']\n",
    "                abstract = data['token_title']\n",
    "                # 写入tf.Example\n",
    "                tf_example = example_pb2.Example()\n",
    "                tf_example.features.feature['article'].bytes_list.value.extend([bytes(article, encoding='utf-8')])\n",
    "                tf_example.features.feature['abstract'].bytes_list.value.extend([bytes(abstract, encoding='utf-8')])\n",
    "                tf_example_str = tf_example.SerializeToString()\n",
    "                str_len = len(tf_example_str)\n",
    "                writer.write(struct.pack('q', str_len))\n",
    "                writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "\n",
    "                # 如果可以，将词典写入文件\n",
    "                if makevocab:\n",
    "                    art_tokens = article.split(' ')\n",
    "                    abs_tokens = abstract.split(' ')\n",
    "                    abs_tokens = [t for t in abs_tokens]  # 从词典中删除这些符号\n",
    "                    tokens = art_tokens + abs_tokens\n",
    "                    tokens = [t.strip() for t in tokens]     # 去掉句子开头结尾的空字符\n",
    "                    tokens = [t for t in tokens if t != \"\"]  # 删除空行\n",
    "                    vocab_counter.update(tokens)\n",
    "    print(\"Finished writing file %s\\n\" % out_file)\n",
    "\n",
    "    # 将词典写入文件\n",
    "    if makevocab:\n",
    "        print(\"Writing vocab file...\")\n",
    "        with open(os.path.join(FINISHED_FILE_DIR, \"vocab\"), 'w', encoding='utf-8') as writer:\n",
    "            for word, count in vocab_counter.most_common(VOCAB_SIZE):\n",
    "                writer.write(word + ' ' + str(count) + '\\n')\n",
    "        print(\"Finished writing vocab file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
